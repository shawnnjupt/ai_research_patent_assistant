import asyncio
import re
from datetime import datetime, timedelta, timezone
from itertools import chain

import aiohttp
from bs4 import BeautifulSoup, NavigableString, Tag
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TimeElapsedColumn
from arxiv_time import next_arxiv_update_day
from paper import Paper, PaperDatabase, PaperExporter
import requests
import os
import json
from pathlib import Path

class PaperDownloader:
    def __init__(self, output_dir="./output_llm"):
        self.output_dir = Path(output_dir)


    def sanitize_filename(self, title: str) -> str:
            """
            æ¸…æ´—è®ºæ–‡æ ‡é¢˜ï¼Œå»é™¤éæ³•æ–‡ä»¶åå­—ç¬¦
            """
            # å»é™¤éæ³•å­—ç¬¦ï¼Œå¹¶é™åˆ¶æœ€å¤§é•¿åº¦
            sanitized = re.sub(r'[<>:"/\\|?*\x00-\x1F]', "", title).strip()
            return sanitized[:200]  # é˜²æ­¢æ–‡ä»¶åè¿‡é•¿

    def get(self, title: str, date: str):
        """
        æ ¹æ®è®ºæ–‡æ ‡é¢˜å’Œæ—¥æœŸä¸‹è½½å¯¹åº”çš„PDFæ–‡ä»¶
        :param title: è®ºæ–‡æ ‡é¢˜
        :param date: æ—¥æœŸæ ¼å¼ä¸º "YYYY-MM-DD"
        """
        # æ„é€ æ–‡ä»¶å
        filename = f"{date}.json"
        file_path = self.output_dir / filename

        if not file_path.exists():
            print(f"æœªæ‰¾åˆ°æ—¥æœŸä¸º {date} çš„ JSON æ–‡ä»¶ã€‚è·¯å¾„ï¼š{file_path}")
            return

        # è¯»å– JSON æ–‡ä»¶
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        # æŸ¥æ‰¾æ ‡é¢˜åŒ¹é…çš„è®ºæ–‡
        chosen_papers = data.get("chosen", [])
        target_paper = None
        for paper in chosen_papers:
            if paper["title"].strip() == title.strip():
                target_paper = paper
                break

        if not target_paper:
            print(f"åœ¨ {date} çš„ 'chosen' ä¸­æœªæ‰¾åˆ°æ ‡é¢˜ä¸º '{title}' çš„è®ºæ–‡ã€‚")
            return

        # æ„é€  PDF é“¾æ¥
        abs_url = target_paper["url"]
        pdf_url = abs_url.replace("/abs/", "/pdf/")

        # ä¸‹è½½ PDF
        print(f"æ­£åœ¨ä¸‹è½½ï¼š{title}")
        response = requests.get(pdf_url, stream=True)
        safe_title = self.sanitize_filename(title)
        if response.status_code == 200:
            # ç”¨è®ºæ–‡IDå‘½åPDFæ–‡ä»¶ï¼ˆä¾‹å¦‚ï¼š2504.18103.pdfï¼‰
            paper_id = abs_url.split("/")[-1]
            pdf_path = Path("downloads_pdf") / f"{safe_title}.pdf"
            pdf_path.parent.mkdir(exist_ok=True)

            with open(pdf_path, "wb") as f:
                for chunk in response.iter_content(chunk_size=1024):
                    if chunk:
                        f.write(chunk)
            print(f"PDF ä¸‹è½½å®Œæˆï¼š{pdf_path}")
        else:
            print(f"ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")



class ArxivScraper(object):
    def __init__(
        self,
        date_from,
        date_until,
        category_blacklist=[],
        category_whitelist=["cs.CV", "cs.AI", "cs.LG", "cs.CL", "cs.IR", "cs.MA"],
        optional_keywords=["LLM", "LLMs", "language model", "language models", "multimodal", "finetuning", "GPT"],
        trans_to="zh-CN",
        proxy=None,
    ):
        """
        ä¸€ä¸ªæŠ“å–æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„arxivæ–‡ç« çš„ç±»,
        æœç´¢åŸºäºhttps://arxiv.org/search/advanced,
        ä¸€ä¸ªæ–‡ä»¶è¢«çˆ¬å–åˆ°çš„æ¡ä»¶æ˜¯ï¼šé¦–æ¬¡æäº¤æ—¶é—´åœ¨`date_from`å’Œ`date_until`ä¹‹é—´ï¼Œå¹¶ä¸”åŒ…å«è‡³å°‘ä¸€ä¸ªå…³é”®è¯ã€‚
        ä¸€ä¸ªæ–‡ç« è¢«è¯¦ç»†å±•ç¤ºï¼ˆä¸è¢«è¿‡æ»¤ï¼‰çš„æ¡ä»¶æ˜¯ï¼šè‡³å°‘æœ‰ä¸€ä¸ªé¢†åŸŸåœ¨ç™½åå•ä¸­ï¼Œå¹¶ä¸”æ²¡æœ‰ä»»ä½•ä¸€ä¸ªé¢†åŸŸåœ¨é»‘åå•ä¸­ã€‚
        ç¿»è¯‘åŸºäºgoogle-translate

        Args:
            date_from (str): å¼€å§‹æ—¥æœŸ(å«å½“å¤©)
            date_until (str): ç»“æŸæ—¥æœŸ(å«å½“å¤©)
            category_blacklist (list, optional): é»‘åå•. Defaults to [].
            category_whitelist (list, optional): ç™½åå•. Defaults to ["cs.CV", "cs.AI", "cs.LG", "cs.CL", "cs.IR", "cs.MA"].
            optional_keywords (list, optional): å…³é”®è¯, å„è¯ä¹‹é—´å…³ç³»ä¸ºOR, åœ¨æ ‡é¢˜/æ‘˜è¦ä¸­è‡³å°‘è¦å‡ºç°ä¸€ä¸ªå…³é”®è¯æ‰ä¼šè¢«çˆ¬å–.
                Defaults to [ "LLM", "LLMs", "language model", "language models", "multimodal", "finetuning", "GPT"]
            trans_to: ç¿»è¯‘çš„ç›®æ ‡è¯­è¨€, è‹¥è®¾ä¸ºå¯è½¬æ¢ä¸ºFalseçš„å€¼åˆ™ä¸ä¼šç¿»è¯‘
            proxy (str | None, optional): ç”¨äºç¿»è¯‘å’Œçˆ¬å–arxivæ—¶è¦ä½¿ç”¨çš„ä»£ç†, é€šå¸¸æ˜¯http://127.0.0.1:7890. Defaults to None
        """
        # announced_date_first æ—¥æœŸå¤„ç†ä¸ºå¹´æœˆï¼Œä»fromåˆ°untilçš„æ‰€æœ‰æœˆä»½éƒ½ä¼šè¢«çˆ¬å–
        # å¦‚æœfromå’Œuntilæ˜¯åŒä¸€ä¸ªæœˆï¼Œåˆ™untilè®¾ç½®ä¸ºä¸‹ä¸ªæœˆ(from+31)
        self.search_from_date = datetime.strptime(date_from[:-3], "%Y-%m")
        self.search_until_date = datetime.strptime(date_until[:-3], "%Y-%m")
        if self.search_from_date.month == self.search_until_date.month:
            self.search_until_date = (self.search_from_date + timedelta(days=31)).replace(day=1)
        # ç”±äºarxivçš„å¥‡æ€ªæœºåˆ¶ï¼Œæ¯ä¸ªæœˆçš„ç¬¬ä¸€å¤©å…¬å¸ƒçš„æ–‡ç« æ€»ä¼šè¢«è§†ä½œä¸Šä¸ªæœˆçš„æ–‡ç« , æ‰€ä»¥éœ€è¦å°†æœˆåˆæ–‡ç« çš„é¦–æ¬¡å…¬å¸ƒæ—¥æœŸå¾€åæ¨ä¸€å¤©
        self.fisrt_announced_date = next_arxiv_update_day(next_arxiv_update_day(self.search_from_date) + timedelta(days=1))

        self.category_blacklist = category_blacklist  # used as metadata
        self.category_whitelist = category_whitelist  # used as metadata
        self.optional_keywords = [kw.replace(" ", "+") for kw in optional_keywords]  # urlè½¬ä¹‰

        self.trans_to = trans_to  # translate
        self.proxy = proxy

        self.filt_date_by = "announced_date_first"  # url
        self.order = "-announced_date_first"  # url(ç»“æœé»˜è®¤æŒ‰é¦–æ¬¡å…¬å¸ƒæ—¥æœŸçš„é™åºæ’åˆ—ï¼Œè¿™æ ·æœ€æ–°å…¬å¸ƒçš„ä¼šåœ¨å‰é¢)
        self.total = None  # fetch_all
        self.step = 50  # url, fetch_all
        self.papers: list[Paper] = []  # fetch_all

        self.paper_db = PaperDatabase()
        self.paper_exporter = PaperExporter(date_from, date_until, category_blacklist, category_whitelist)
        self.console = Console()

    @property
    def meta_data(self):
        """
        è¿”å›æœç´¢çš„å…ƒæ•°æ®
        """
        return dict(repo_url="https://github.com", **self.__dict__)

    def get_url(self, start):
        """
        è·å–ç”¨äºæœç´¢çš„url

        Args:
            start (int): è¿”å›ç»“æœçš„èµ·å§‹åºå·, æ¯ä¸ªé¡µé¢åªä¼šåŒ…å«åºå·ä¸º[start, start+50)çš„æ–‡ç« 
            filter_date_by (str, optional): æ—¥æœŸç­›é€‰æ–¹å¼. Defaults to "submitted_date_first".
        """
        # https://arxiv.org/search/advanced?terms-0-operator=AND&terms-0-term=LLM&terms-0-field=all&terms-1-operator=OR&terms-1-term=language+model&terms-1-field=all&terms-2-operator=OR&terms-2-term=multimodal&terms-2-field=all&terms-3-operator=OR&terms-3-term=finetuning&terms-3-field=all&terms-4-operator=AND&terms-4-term=GPT&terms-4-field=all&classification-computer_science=y&classification-physics_archives=all&classification-include_cross_list=include&date-year=&date-filter_by=date_range&date-from_date=2024-08-08&date-to_date=2024-08-15&date-date_type=submitted_date_first&abstracts=show&size=50&order=submitted_date
        kwargs = "".join(
            f"&terms-{i}-operator=OR&terms-{i}-term={kw}&terms-{i}-field=all"
            for i, kw in enumerate(self.optional_keywords)
        )
        date_from = self.search_from_date.strftime("%Y-%m")
        date_until = self.search_until_date.strftime("%Y-%m")
        return (
            f"https://arxiv.org/search/advanced?advanced={kwargs}"
            f"&classification-computer_science=y&classification-physics_archives=all&"
            f"classification-include_cross_list=include&"
            f"date-year=&date-filter_by=date_range&date-from_date={date_from}&date-to_date={date_until}&"
            f"date-date_type={self.filt_date_by}&abstracts=show&size={self.step}&order={self.order}&start={start}"
        )
    async def request(self, start):
        """
        å¼‚æ­¥è¯·æ±‚ç½‘é¡µï¼Œé‡è¯•è‡³å¤š3æ¬¡
        """
        error = 0
        url = self.get_url(start)
        while error <= 3:
            try:
                async with aiohttp.ClientSession(trust_env=True, read_timeout=10) as session:
                    async with session.get(url, proxy=self.proxy) as response:
                        response.raise_for_status()
                        content = await response.text()
                        return content
            except Exception as e:
                error += 1
                self.console.log(f"[bold red]Request {start} cause error: ")
                self.console.print_exception()
                self.console.log(f"[bold red]Retrying {start}... {error}/3")

    async def fetch_all(self):
        """
        (aio)è·å–æ‰€æœ‰æ–‡ç« 
        """
        # è·å–å‰50ç¯‡æ–‡ç« å¹¶è®°å½•æ€»æ•°
        self.console.log(f"[bold green]Fetching the first {self.step} papers...")
        self.console.print(f"[grey] {self.get_url(0)}")
        content = await self.request(0)
        self.papers.extend(self.parse_search_html(content))

        # è·å–å‰©ä½™çš„å†…å®¹
        with Progress(
            SpinnerColumn(),
            *Progress.get_default_columns(),
            TimeElapsedColumn(),
            console=self.console,
            transient=False,
        ) as p:  # richè¿›åº¦æ¡
            task = p.add_task(
                description=f"[bold green]Fetching {self.total} results",
                total=self.total,
            )
            p.update(task, advance=self.step)

            async def wrapper(start):  # wrapperç”¨äºæ˜¾ç¤ºè¿›åº¦
                # å¼‚æ­¥è¯·æ±‚ç½‘é¡µï¼Œå¹¶è§£æå…¶ä¸­çš„å†…å®¹
                content = await self.request(start)
                papers = self.parse_search_html(content)
                p.update(task, advance=self.step)
                return papers

            # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
            fetch_tasks = []
            for start in range(self.step, self.total, self.step):
                fetch_tasks.append(wrapper(start))
            papers_list = await asyncio.gather(*fetch_tasks)
            self.papers.extend(chain(*papers_list))

        self.console.log(f"[bold green]Fetching completed. ")
        if self.trans_to:
            await self.translate()
        self.process_papers()

    def fetch_update(self):
        """
        æ›´æ–°æ–‡ç« , è¿™ä¼šä»æœ€æ–°å…¬å¸ƒçš„æ–‡ç« å¼€å§‹æ›´æ–°, ç›´åˆ°é‡åˆ°å·²ç»çˆ¬å–è¿‡çš„æ–‡ç« ä¸ºæ­¢ã€‚
        ä¸ºäº†æ•ˆç‡,å»ºè®®åœ¨è¿è¡Œfetch_allåå†è¿è¡Œfetch_update
        """
        # å½“å‰æ—¶é—´
        utc_now = datetime.now(timezone.utc).replace(tzinfo=None)
        # ä¸Šä¸€æ¬¡æ›´æ–°æœ€æ–°æ–‡ç« çš„UTCæ—¶é—´. é™¤äº†æ›´æ–°æ–°æ–‡ç« å¤–ä¹Ÿå¯èƒ½é‡æ–°çˆ¬å–äº†è€æ–‡ç« , æ•°æ®åº“åªçœ‹æœ€æ–°æ–‡ç« çš„æ—¶é—´æˆ³ã€‚
        last_update = self.paper_db.newest_update_time()
        # æ£€æŸ¥ä¸€ä¸‹ä¸Šæ¬¡ä¹‹åçš„æœ€è¿‘ä¸€ä¸ªarxivæ›´æ–°æ—¥æœŸ
        self.search_from_date = next_arxiv_update_day(last_update)
        self.console.log(f"[bold yellow]last update: {last_update.strftime('%Y-%m-%d %H:%M:%S')}, "
                         f"next arxiv update: {self.search_from_date.strftime('%Y-%m-%d')}" 
                         )
        self.console.log(f"[bold yellow]UTC now: {utc_now.strftime('%Y-%m-%d %H:%M:%S')}")
        # å¦‚æœè¿˜æ²¡åˆ°æ›´æ–°æ—¶é—´å°±ä¸æ›´æ–°äº†
        if self.search_from_date >= utc_now:
            self.console.log(f"[bold red]Your database is already up to date.")
            return
        # å¦‚æœè¿™ä¸€æ¬¡çš„æ›´æ–°æ—¶é—´æ°å¥½æ˜¯è¿™ä¸ªæœˆçš„ç¬¬ä¸€ä¸ªæ›´æ–°æ—¥ï¼Œé‚£ä¹ˆå½“æ—¥æ›´æ–°çš„æ–‡ç« éƒ½ä¼šå‡ºç°åœ¨ä¸Šä¸ªæœˆçš„æœç´¢ç»“æœä¸­
        # ä¸ºäº†æ­£ç¡®è·å¾—è¿™å¤©çš„æ–‡ç« ï¼Œæˆ‘ä»¬ä¸Šæ¨ä¸€ä¸ªæœˆçš„æœç´¢æ—¶é—´
        self.fisrt_announced_date = self.search_from_date
        if self.search_from_date == next_arxiv_update_day(self.search_from_date.replace(day=1)):
            self.search_from_date = self.search_from_date - timedelta(days=31)
            self.console.log(f"[bold yellow]The update in {self.fisrt_announced_date.strftime('%Y-%m-%d')} can only be found in the previous month.")
        else:
            self.console.log(
                f"[bold green]Searching from {self.search_from_date.strftime('%Y-%m-%d')} "
                f"to {self.search_until_date.strftime('%Y-%m-%d')}, fetch the first {self.step} papers..."
            )
        self.console.print(f"[grey] {self.get_url(0)}")

        continue_update = self.update(0)
        for start in range(self.step, self.total, self.step):
            if not continue_update:
                break

            continue_update = self.update(start)
        self.console.log(f"[bold green]Fetching completed. {len(self.papers)} new papers.")
        if self.trans_to:
            asyncio.run(self.translate())
        self.process_papers()

    def process_papers(self):
        """
        æ¨æ–­æ–‡ç« çš„é¦–æ¬¡å…¬å¸ƒæ—¥æœŸ, å¹¶å°†æ–‡ç« æ·»åŠ åˆ°æ•°æ®åº“ä¸­
        """
        # ä»ä¸‹ä¸€ä¸ªå¯èƒ½çš„å…¬å¸ƒæ—¥æœŸå¼€å§‹
        announced_date = next_arxiv_update_day(self.fisrt_announced_date)   
        self.console.log(f"fisrt announced date: {announced_date.strftime('%Y-%m-%d')}")
        # æŒ‰ç…§ä»å‰åˆ°åçš„æ—¶é—´é¡ºåºæ¢³ç†æ–‡ç« 
        for paper in reversed(self.papers):
            # æ–‡ç« äºTæ—¥ç¾ä¸œæ—¶é—´14:00(T UTC+0 18:00)å‰æäº¤ï¼Œå°†äºTæ—¥ç¾ä¸œæ—¶é—´20:00(T+1 UTC+0 00:00)å…¬å¸ƒï¼ŒTå§‹ç»ˆä¸ºå·¥ä½œæ—¥ã€‚
            # å› æ­¤å¯çŸ¥ç¾ä¸œ Tæ—¥çš„æ–‡ç« è‡³å°‘åœ¨UTC+0 T+1æ—¥å…¬å¸ƒï¼Œå¦‚æœè¶…è¿‡14:00ç”šè‡³ä¼šåœ¨UTC+0 T+2æ—¥å…¬å¸ƒ
            next_possible_annouced_date = next_arxiv_update_day(paper.first_submitted_date + timedelta(days=1))
            if announced_date < next_possible_annouced_date:
                announced_date = next_possible_annouced_date
            paper.first_announced_date = announced_date
        self.paper_db.add_papers(self.papers)
    
    def reprocess_papers(self):
        """
        è¿™ä¼šä»æ•°æ®åº“ä¸­è·å–æ‰€æœ‰æ–‡ç« , å¹¶é‡æ–°æ¨æ–­æ–‡ç« çš„é¦–æ¬¡å…¬å¸ƒæ—¥æœŸï¼Œå¹¶æ‰“å°è°ƒè¯•ä¿¡æ¯
        """
        self.papers = self.paper_db.fetch_all()
        self.process_papers()
        with open("announced_date.csv", "w") as f:
            f.write("url,title,announced_date,submitted_date\n")
            for paper in self.papers:
                f.write(
                    f"{paper.url},{paper.title},{paper.first_announced_date.strftime('%Y-%m-%d')},{paper.first_submitted_date.strftime('%Y-%m-%d')}\n"
                )

    def update(self, start) -> bool:
        content = asyncio.run(self.request(start))
        self.papers.extend(self.parse_search_html(content))
        cnt_new = self.paper_db.count_new_papers(self.papers[start : start + self.step])
        if cnt_new < self.step:
            self.papers = self.papers[: start + cnt_new]
            return False
        else:
            return True

    def parse_search_html(self, content) -> list[Paper]:
        """
        è§£ææœç´¢ç»“æœé¡µé¢, å¹¶å°†ç»“æœä¿å­˜åˆ°self.paper_resultä¸­
        åˆæ¬¡è°ƒç”¨æ—¶, ä¼šè§£æself.total

        Args:
            content (str): ç½‘é¡µå†…å®¹
        """

        """ä¸‹é¢æ˜¯ä¸€ä¸ªæœç´¢ç»“æœçš„ä¾‹å­
        <li class="arxiv-result">
            <div class="is-marginless">
                <p class="list-title is-inline-block">
                    <a href="https://arxiv.org/abs/physics/9403001">arXiv:physics/9403001</a>
                    <span>&nbsp;[<a href="https://arxiv.org/pdf/physics/9403001">pdf</a>, <a
                            href="https://arxiv.org/ps/physics/9403001">ps</a>, <a
                            href="https://arxiv.org/format/physics/9403001">other</a>]&nbsp;</span>
                </p>
                <div class="tags is-inline-block">
                    <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Popular Physics">
                        physics.pop-ph</span>
                    <span class="tag is-small is-grey tooltip is-tooltip-top"
                        data-tooltip="High Energy Physics - Theory">hep-th</span>
                </div>
                <div class="is-inline-block" style="margin-left: 0.5rem">
                    <div class="tags has-addons">
                        <span class="tag is-dark is-size-7">doi</span>
                        <span class="tag is-light is-size-7">
                            <a class="" href="https://doi.org/10.1063/1.2814991">10.1063/1.2814991 <i
                                    class="fa fa-external-link" aria-hidden="true"></i></a>
                        </span>
                    </div>
                </div> 
            </div>
            <p class="title is-5 mathjax">
                Desperately Seeking Superstrings
            </p>
            <p class="authors">
                <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
                    <a href="/search/?searchtype=author&amp;query=Ginsparg%2C+P">Paul Ginsparg</a>, <a href="/search/?searchtype=author&amp;query=Glashow%2C+S">Sheldon Glashow</a> 
            </p> 
            <p class="abstract mathjax">
                <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: 
                
                <span class="abstract-short has-text-grey-dark mathjax" id="physics/9403001v1-abstract-short"
                    style="display: inline;"> We provide a detailed analysis of the problems and prospects of superstring theory c.
                1986, anticipating much of the progress of the decades to follow. </span>

                <span class="abstract-full has-text-grey-dark mathjax" id="physics/9403001v1-abstract-full"
                    style="display: none;"> We provide a detailed analysis of the problems and prospects of
                superstring theory c. 1986, anticipating much of the progress of the decades to follow. 
                <a class="is-size-7" style="white-space: nowrap;"
                        onclick="document.getElementById('physics/9403001v1-abstract-full').style.display = 'none'; document.getElementById('physics/9403001v1-abstract-short').style.display = 'inline';">â–³ Less</a>
                </span>
            </p> 
            <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span>
                25 April, 1986; <span class="has-text-black-bis has-text-weight-semibold">originally
                announced</span> March 1994. </p> 
            <p class="comments is-size-7">
                <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
                <span class="has-text-grey-dark mathjax">originally appeared as a Reference Frame in Physics
                    Today, May 1986</span>
            </p> 
            <p class="comments is-size-7">
                <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span> Phys.Today
                86N5 (1986) 7-9 </p> 
        </li>
        """

        soup = BeautifulSoup(content, "html.parser")
        if not self.total:
            total = soup.select("#main-container > div.level.is-marginless > div.level-left > h1")[0].text
            # "Showing 1â€“50 of 2,542,002 results" or "Sorry, your query returned no results"
            if "Sorry" in total:
                self.total = 0
                return []
            total = int(total[total.find("of") + 3 : total.find("results")].replace(",", ""))
            self.total = total

        results = soup.find_all("li", {"class": "arxiv-result"})
        papers = []
        for result in results:

            url_tag = result.find("a")
            url = url_tag["href"] if url_tag else "No link"
            # print(url)

            title_tag = result.find("p", class_="title")
            title = self.parse_search_text(title_tag) if title_tag else "No title"
            title = title.strip()

            date_tag = result.find("p", class_="is-size-7")
            date = date_tag.get_text(strip=True) if date_tag else "No date"
            if "v1" in date:
                # Submitted9 August, 2024; v1submitted 8 August, 2024; originally announced August 2024.
                # æ³¨æ„ç©ºæ ¼ä¼šè¢«åæ‰ï¼Œè¿™é‡Œæˆ‘ä»¬è¦æ‰¾æœ€æ—©çš„æäº¤æ—¥æœŸ
                v1 = date.find("v1submitted")
                date = date[v1 + 12 : date.find(";", v1)]
            else:
                # Submitted8 August, 2024; originally announced August 2024.
                # æ³¨æ„ç©ºæ ¼ä¼šè¢«åæ‰
                submit_date = date.find("Submitted")
                date = date[submit_date + 9 : date.find(";", submit_date)]

            category_tag = result.find_all("span", class_="tag")
            categories = [
                category.get_text(strip=True) for category in category_tag if "tooltip" in category.get("class")
            ]

            authors_tag = result.find("p", class_="authors")
            authors = authors_tag.get_text(strip=True)[len("Authors:") :] if authors_tag else "No authors"

            summary_tag = result.find("span", class_="abstract-full")
            abstract = self.parse_search_text(summary_tag) if summary_tag else "No summary"
            abstract = abstract.strip()

            comments_tag = result.find("p", class_="comments")
            comments = comments_tag.get_text(strip=True)[len("Comments:") :] if comments_tag else "No comments"

            papers.append(
                Paper(
                    url=url,
                    title=title,
                    first_submitted_date=datetime.strptime(date, "%d %B, %Y"),
                    categories=categories,
                    authors=authors,
                    abstract=abstract,
                    comments=comments,
                )
            )
        return papers

    def parse_search_text(self, tag):
        string = ""
        for child in tag.children:
            if isinstance(child, NavigableString):
                string += re.sub(r"\s+", " ", child)
            elif isinstance(child, Tag):
                if child.name == "span" and "search-hit" in child.get("class"):
                    string += re.sub(r"\s+", " ", child.get_text(strip=False))
                elif child.name == "a" and ".style.display" in child.get("onclick"):
                    pass
                else:
                    import pdb

                    pdb.set_trace()
        return string

    async def translate(self):
        if not self.trans_to:
            raise ValueError("No target language specified.")
        self.console.log("[bold green]Translating...")
        with Progress(
            SpinnerColumn(),
            *Progress.get_default_columns(),
            TimeElapsedColumn(),
            console=self.console,
            transient=False,
        ) as p:
            total = len(self.papers)
            task = p.add_task(
                description=f"[bold green]Translating {total} papers",
                total=total,
            )

            async def worker(paper):
                await paper.translate(langto=self.trans_to)
                p.update(task, advance=1)

            await asyncio.gather(*[worker(paper) for paper in self.papers])

    def to_markdown(self, output_dir="./output_llms", filename_format="%Y-%m-%d", meta=False):
        self.paper_exporter.to_markdown(output_dir, filename_format, self.meta_data if meta else None)
    def to_json(self, output_dir="./output_llms", filename_format="%Y-%m-%d", meta=False):
        self.paper_exporter.to_json(output_dir, filename_format, self.meta_data if meta else None)
    def to_csv(self, output_dir="./output_llms", filename_format="%Y-%m-%d",  header=False, csv_config={},):
        self.paper_exporter.to_csv(output_dir, filename_format, header, csv_config)


if __name__ == "__main__":
    from datetime import date, timedelta

    today = date.today()

    scraper = ArxivScraper(
        date_from="2025-04-22",
        date_until="2025-04-29",
        category_whitelist=["cs.AI", "cs.AR", "cs.CL", "cs.IR","cs.CV"],
        optional_keywords=["GNN"],
        trans_to=False,
        proxy="http://127.0.0.1:7890"
    )
    asyncio.run(scraper.fetch_all())
    # scraper.to_markdown(meta=True)
    scraper.to_json(meta=True)
    # ğŸ‘‡ å…³é—­æ‰€æœ‰æ•°æ®åº“è¿æ¥
    scraper.paper_db.conn.close() 
    if hasattr(scraper.paper_exporter, 'db'):
        scraper.paper_exporter.db.conn.close()
     
    db_file = "./papers.db"
    if os.path.exists(db_file):
        os.remove(db_file)
        print(f"[INFO] æ•°æ®åº“æ–‡ä»¶ {db_file} å·²åˆ é™¤ã€‚")
    else:
        print(f"[INFO] æ•°æ®åº“æ–‡ä»¶ {db_file} ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤ã€‚")

    # downloader = PaperDownloader(output_dir="./output_llms")
    # downloader.get(
    #     title="QAOA-GPT: Efficient Generation of Adaptive and Regular Quantum Approximate Optimization Algorithm Circuits",
    #     date="2025-04-23"
    # )
