import os
import json
import re
from collections import Counter, defaultdict
import requests
from tqdm import tqdm




def extract_keywords_with_deepseek(text):
    """ä½¿ç”¨ DeepSeek API æå–é›†æˆç”µè·¯é¢†åŸŸä¸“ä¸šå…³é”®è¯"""
    headers = {
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
        "Content-Type": "application/json"
    }

    prompt = f"""
è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–é›†æˆç”µè·¯/åŠå¯¼ä½“ç›¸å…³çš„ä¸“ä¸šæœ¯è¯­æˆ–æŠ€æœ¯å…³é”®è¯ã€‚
å¦‚æœæ˜¯è‹±æ–‡ï¼Œè¯·ç¿»è¯‘æˆä¸­æ–‡ï¼›å¦‚æœæ˜¯ä¸­æ–‡ï¼Œç›´æ¥æå–å³å¯ã€‚
åªè¿”å›å…³é”®è¯åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦è§£é‡Šæˆ–å…¶ä»–å†…å®¹ã€‚

æ–‡æœ¬å†…å®¹ï¼š
{text}
"""

    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3,
        "top_p": 0.1,
        "max_tokens": 150
    }

    try:
        response = requests.post(DEEPSEEK_API_URL, json=payload, headers=headers)
        response.raise_for_status()
        content = response.json()['choices'][0]['message']['content'].strip()
        return [line.strip() for line in content.split('\n') if line.strip()]
    except Exception as e:
        print(f"APIè°ƒç”¨å¤±è´¥: {e}")
        return []


def read_all_json_files(folder_path):
    all_papers = []

    json_files = [f for f in os.listdir(folder_path) if f.endswith(".json")]
    print("ğŸ“š æ­£åœ¨è¯»å– JSON æ–‡ä»¶...")

    for filename in tqdm(json_files, desc="ğŸ“„ è¯»å–è¿›åº¦", unit="file"):
        file_path = os.path.join(folder_path, filename)

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

                # æ£€æŸ¥æ˜¯å¦ä¸ºé¢„æœŸç»“æ„
                if not isinstance(data, dict):
                    print(f"âš ï¸ {filename} ä¸­çš„æ•°æ®ä¸æ˜¯å­—å…¸ç±»å‹")
                    continue

                # æå– "chosen" åˆ—è¡¨ä¸­çš„æ¯ç¯‡è®ºæ–‡
                papers = data.get("chosen", [])
                if not isinstance(papers, list):
                    print(f"âš ï¸ {filename} ä¸­çš„ 'chosen' ä¸æ˜¯åˆ—è¡¨ç±»å‹")
                    continue

                for paper in papers:
                    if isinstance(paper, dict):
                        # æ·»åŠ å¹´ä»½ä¿¡æ¯ï¼ˆä» date å­—æ®µæå–ï¼‰
                        date_str = data.get("date", "")
                        if date_str.startswith(("202", "203")):  # å¦‚ "2025-04-22"
                            year = int(date_str.split("-")[0])
                            paper["assigned_year"] = year
                        all_papers.append(paper)
                    else:
                        print(f"âš ï¸ è·³è¿‡éå­—å…¸ç±»å‹çš„è®ºæ–‡ï¼š{repr(paper)}")

        except Exception as e:
            print(f"âš ï¸ è¯»å– {filename} å¤±è´¥: {e}")

    return all_papers


def extract_keywords_per_article_from_patents(patents):
    """ä»è®ºæ–‡æ•°æ®ä¸­æå–æ¯ç¯‡æ–‡ç« çš„å…³é”®è¯"""
    all_keywords_list = []

    print("ğŸ§  æ­£åœ¨æå–æ¯ç¯‡è®ºæ–‡çš„å…³é”®è¯...")
    for patent in tqdm(patents, desc="ğŸ” æå–å…³é”®è¯è¿›åº¦", unit="patent"):
        title = patent.get("title", "")
        abstract = patent.get("abstract", "")

        text = title + "ã€‚" + abstract
        if len(text.strip()) < 30:
            continue

        keywords = extract_keywords_with_deepseek(text)
        if keywords:
            all_keywords_list.append(keywords)

    return all_keywords_list


def build_keyword_frequency(all_keywords_list):
    """ç»Ÿè®¡æ¯ä¸ªå…³é”®è¯çš„å‡ºç°é¢‘ç‡"""
    keyword_counter = Counter()
    for keywords in all_keywords_list:
        keyword_counter.update(set(keywords))  # æ¯ç¯‡åªè®¡ä¸€æ¬¡
    return keyword_counter


def build_cooccurrence_matrix(all_keywords_list):
    """æ„å»ºå…³é”®è¯å…±ç°çŸ©é˜µ"""
    co_occurrence = Counter()

    for keywords in all_keywords_list:
        keywords = sorted(set(keywords))  # å»é‡å¹¶æ’åºä»¥é¿å…é‡å¤å¯¹
        for i in range(len(keywords)):
            for j in range(i + 1, len(keywords)):
                pair = tuple(sorted((keywords[i], keywords[j])))
                co_occurrence[pair] += 1

    return co_occurrence


import random

# å¯ä»¥æ ¹æ®éœ€è¦è‡ªå®šä¹‰é¢œè‰²ç»„
GROUP_COLORS = ["red", "blue", "green", "yellow"]

import random

# å°† group æ”¹ä¸ºæ•°å­— 1~4
GROUP_VALUES = [1, 2, 3, 4]

def generate_graph_data(keyword_freq, co_occurrence, top_n=30):
    """ç”Ÿæˆå›¾è°±æ•°æ®ç»“æ„ï¼Œå¹¶æ”¯æŒæ§åˆ¶è¾“å‡ºèŠ‚ç‚¹æ•°é‡å’Œéšæœº group åˆ†ç»„"""
    # æŒ‰ç…§é¢‘ç‡æ’åºå¹¶é€‰å–å‰top_nä¸ªå…³é”®è¯
    sorted_keywords = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)[:top_n]
    keyword_set = set(kw for kw, _ in sorted_keywords)

    nodes = []
    links = []

    # æ„å»ºèŠ‚ç‚¹æ•°æ®ï¼Œå¹¶éšæœºåˆ†é… group å€¼ï¼ˆ1~4ï¼‰
    for keyword, freq in sorted_keywords:
        nodes.append({
            "id": keyword,
            "group": random.choice(GROUP_VALUES)  # æ·»åŠ éšæœºåˆ†ç»„ç¼–å·
        })

    # æ„å»ºè¿çº¿æ•°æ®ï¼Œå¹¶éšæœºæŒ‡å®švalueå€¼
    for (source, target), weight in co_occurrence.items():
        if source in keyword_set and target in keyword_set:
            links.append({
                "source": source,
                "target": target,
                "value": random.randint(1, 10)  # çº¿å®½éšæœºè®¾å®šåœ¨1-10ä¹‹é—´
            })


    return {"nodes": nodes, "links": ensure_connected_graph(nodes, links)}


def save_graph_data_to_json(graph_data, output_path):
    """ä¿å­˜ä¸º JSON æ–‡ä»¶"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(graph_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… å›¾è°±æ•°æ®å·²ä¿å­˜è‡³ {output_path}")

def ensure_connected_graph(nodes, links):
    """
    ç¡®ä¿å›¾æ˜¯è¿é€šçš„ï¼šé€‰æ‹©ä¸€ä¸ªä¸­å¿ƒèŠ‚ç‚¹ï¼Œå°†å…¶ä¸æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹è¿æ¥
    """
    node_ids = [node["id"] for node in nodes]
    if len(node_ids) <= 1:
        return links

    central_node = node_ids[0]  # é€‰æ‹©ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ä½œä¸ºä¸­å¿ƒèŠ‚ç‚¹
    existing_pairs = set((link["source"], link["target"]) for link in links)

    for node_id in node_ids[1:]:
        if (central_node, node_id) not in existing_pairs and (node_id, central_node) not in existing_pairs:
            links.append({
                "source": central_node,
                "target": node_id,
                "value": random.randint(1, 10)  # éšæœºèµ‹å€¼
            })

    return links

if __name__ == "__main__":
    folder_path = 'D:/ai/output_llms'  # æ”¹ä¸ºä½ è‡ªå·±çš„è·¯å¾„
    output_path = 'graph_data.json'

    # Step 1: è¯»å–æ‰€æœ‰è®ºæ–‡æ•°æ®
    papers = read_all_json_files(folder_path)

    # Step 2: æå–æ¯ç¯‡è®ºæ–‡çš„å…³é”®è¯
    all_keywords_list = extract_keywords_per_article_from_patents(papers)

    # Step 3: ç»Ÿè®¡å…³é”®è¯é¢‘ç‡
    keyword_freq = build_keyword_frequency(all_keywords_list)

    # Step 4: æ„å»ºå…±ç°çŸ©é˜µ
    co_occurrence = build_cooccurrence_matrix(all_keywords_list)

    # Step 5: ç”Ÿæˆå›¾è°±æ•°æ®
    # Step 5: ç”Ÿæˆå›¾è°±æ•°æ®ï¼Œè¿™é‡Œå¯ä»¥æŒ‡å®šè¾“å‡ºçš„å…³é”®è¯æ•°é‡ï¼Œé»˜è®¤æ˜¯30
    graph_data = generate_graph_data(keyword_freq, co_occurrence, top_n=20)

    # Step 6: ä¿å­˜ä¸º JSON æ–‡ä»¶
    save_graph_data_to_json(graph_data, output_path)